{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWxKpHRHItdk",
        "outputId": "6de64e19-e470-4034-8056-1722f5cea063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EMO-AffectNetModel'...\n",
            "remote: Enumerating objects: 198, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 198 (delta 53), reused 46 (delta 46), pack-reused 138 (from 1)\u001b[K\n",
            "Receiving objects: 100% (198/198), 200.85 MiB | 33.20 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ElenaRyumina/EMO-AffectNetModel.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTODo7ilI47s",
        "outputId": "862823d5-21b2-4e3d-8b08-6c63f7c9ab5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'torch torchvision torchaudio': Expected end or semicolon (after name and no valid version specifier)\n",
            "    torch torchvision torchaudio\n",
            "          ^ (from line 8 of requirements.txt)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv6RHVRUJt_5",
        "outputId": "d07fea0d-41f2-4033-acb5-a29718b86e16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EMO-AffectNetModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLfqRhzcN0M4",
        "outputId": "e55612dc-f129-42c0-8bfe-7d7cf8ae132f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EMO-AffectNetModel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/18e7ODzjngAd_UBf2_AwMRQ1Q6bts1s7E/view?usp=drive_link"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iudev5JgOg_Z",
        "outputId": "b57c64c5-74a4-4a9f-e433-1fa64ab8b3bc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18e7ODzjngAd_UBf2_AwMRQ1Q6bts1s7E\n",
            "To: /content/EMO-AffectNetModel/RAVDESS_with_config.h5\n",
            "100% 11.6M/11.6M [00:00<00:00, 33.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code = r\"\"\"\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from scipy import stats\n",
        "from functions import sequences\n",
        "from functions import get_face_areas\n",
        "from functions.get_models import load_weights_EE, load_weights_LSTM\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category = FutureWarning)\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"run\")\n",
        "\n",
        "parser.add_argument('--path_video', type=str, default='video/', help='Path to all videos')\n",
        "parser.add_argument('--path_save', type=str, default='report/', help='Path to save the report')\n",
        "parser.add_argument('--conf_d', type=float, default=0.7, help='Elimination threshold for false face areas')\n",
        "parser.add_argument('--path_FE_model', type=str, default='models/EmoAffectnet/weights_0_66_37_wo_gl.h5',\n",
        "                    help='Path to a model for feature extraction')\n",
        "parser.add_argument('--path_LSTM_model', type=str, default='models/LSTM/RAVDESS_with_config.h5',\n",
        "                    help='Path to a model for emotion prediction')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "def pred_one_video(path):\n",
        "    start_time = time.time()\n",
        "    label_model = ['Neutral', 'Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
        "    detect = get_face_areas.VideoCamera(path_video=path, conf=args.conf_d)\n",
        "    dict_face_areas, total_frame = detect.get_frame()\n",
        "    name_frames = list(dict_face_areas.keys())\n",
        "    face_areas = list(dict_face_areas.values())\n",
        "    EE_model = load_weights_EE(args.path_FE_model)\n",
        "    LSTM_model = load_weights_LSTM(args.path_LSTM_model)\n",
        "    features = EE_model(np.stack(face_areas))\n",
        "    seq_paths, seq_features = sequences.sequences(name_frames, features)\n",
        "    pred = LSTM_model(np.stack(seq_features)).numpy()\n",
        "    all_pred = []\n",
        "    all_path = []\n",
        "    for id, c_p in enumerate(seq_paths):\n",
        "        c_f = [str(i).zfill(6) for i in range(int(c_p[0]), int(c_p[-1])+1)]\n",
        "        c_pr = [pred[id]]*len(c_f)\n",
        "        all_pred.extend(c_pr)\n",
        "        all_path.extend(c_f)\n",
        "    m_f = [str(i).zfill(6) for i in range(int(all_path[-1])+1, total_frame+1)]\n",
        "    m_p = [all_pred[-1]]*len(m_f)\n",
        "\n",
        "    df=pd.DataFrame(data=all_pred+m_p, columns=label_model)\n",
        "    df['frame'] = all_path+m_f\n",
        "    df = df[['frame']+ label_model]\n",
        "    df = sequences.df_group(df, label_model)\n",
        "\n",
        "    if not os.path.exists(args.path_save):\n",
        "        os.makedirs(args.path_save)\n",
        "\n",
        "    filename = os.path.basename(path)[:-4] + '.csv'\n",
        "    df.to_csv(os.path.join(args.path_save,filename), index=False)\n",
        "    end_time = time.time() - start_time\n",
        "    mode = stats.mode(np.argmax(pred, axis=1))[0]\n",
        "    print('Report saved in: ', os.path.join(args.path_save,filename))\n",
        "    print('Predicted emotion: ', label_model[mode])\n",
        "    print('Lead time: {} s'.format(np.round(end_time, 2)))\n",
        "    print()\n",
        "\n",
        "def pred_all_video():\n",
        "    path_all_videos = [\n",
        "        f for f in os.listdir(args.path_video)\n",
        "        if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))  # filter video files\n",
        "    ]\n",
        "    for id, cr_path in enumerate(path_all_videos):\n",
        "        print('{}/{}'.format(id+1, len(path_all_videos)))\n",
        "        pred_one_video(os.path.join(args.path_video, cr_path))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pred_all_video()\n",
        "\"\"\"\n",
        "with open (\"/content/EMO-AffectNetModel/run.py\",\"w\") as fin:\n",
        "  fin.write(code)\n",
        "print(f\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9nErtXDN5g_",
        "outputId": "c397afa9-294e-42cc-8d7c-640f4abc80bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#edited for Cpu runtime\n",
        "code = r\"\"\"\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from scipy import stats\n",
        "from functions import sequences\n",
        "from functions import get_face_areas\n",
        "from functions.get_models import load_weights_EE, load_weights_LSTM\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"run\")\n",
        "\n",
        "parser.add_argument('--path_video', type=str, default='video/', help='Path to all videos')\n",
        "parser.add_argument('--path_save', type=str, default='report/', help='Path to save the report')\n",
        "parser.add_argument('--conf_d', type=float, default=0.7, help='Elimination threshold for false face areas')\n",
        "parser.add_argument('--path_FE_model', type=str, default='models/EmoAffectnet/weights_0_66_37_wo_gl.h5',\n",
        "                    help='Path to a model for feature extraction')\n",
        "parser.add_argument('--path_LSTM_model', type=str, default='models/LSTM/RAVDESS_with_config.h5',\n",
        "                    help='Path to a model for emotion prediction')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def pred_one_video(path):\n",
        "    start_time = time.time()\n",
        "    label_model = ['Neutral', 'Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger']\n",
        "\n",
        "    detect = get_face_areas.VideoCamera(path_video=path, conf=args.conf_d)\n",
        "    dict_face_areas, total_frame = detect.get_frame()\n",
        "    if total_frame == 0 or len(dict_face_areas) == 0:\n",
        "        print(f\"âš ï¸ Skipping {path} (no frames detected or unreadable video)\")\n",
        "        return\n",
        "\n",
        "    name_frames = list(dict_face_areas.keys())\n",
        "    face_areas = list(dict_face_areas.values())\n",
        "\n",
        "    EE_model = load_weights_EE(args.path_FE_model)\n",
        "    LSTM_model = load_weights_LSTM(args.path_LSTM_model)\n",
        "\n",
        "    features = EE_model(np.stack(face_areas))\n",
        "    seq_paths, seq_features = sequences.sequences(name_frames, features)\n",
        "\n",
        "    pred = LSTM_model(np.stack(seq_features)).numpy()\n",
        "\n",
        "    all_pred = []\n",
        "    all_path = []\n",
        "    for id, c_p in enumerate(seq_paths):\n",
        "        c_f = [str(i).zfill(6) for i in range(int(c_p[0]), int(c_p[-1]) + 1)]\n",
        "        c_pr = [pred[id]] * len(c_f)\n",
        "        all_pred.extend(c_pr)\n",
        "        all_path.extend(c_f)\n",
        "\n",
        "    # fill missing frames with last prediction\n",
        "    m_f = [str(i).zfill(6) for i in range(int(all_path[-1]) + 1, total_frame + 1)]\n",
        "    m_p = [all_pred[-1]] * len(m_f)\n",
        "\n",
        "    df = pd.DataFrame(data=all_pred + m_p, columns=label_model)\n",
        "    df['frame'] = all_path + m_f\n",
        "    df = df[['frame'] + label_model]\n",
        "    df = sequences.df_group(df, label_model)\n",
        "\n",
        "    if not os.path.exists(args.path_save):\n",
        "        os.makedirs(args.path_save)\n",
        "\n",
        "    filename = os.path.basename(path)[:-4] + '.csv'\n",
        "    df.to_csv(os.path.join(args.path_save, filename), index=False)\n",
        "\n",
        "    end_time = time.time() - start_time\n",
        "    mode = stats.mode(np.argmax(pred, axis=1), keepdims=True)[0][0]\n",
        "    print('âœ… Report saved in:', os.path.join(args.path_save, filename))\n",
        "    print('ðŸŽ­ Predicted emotion:', label_model[mode])\n",
        "    print('â±ï¸ Lead time: {} s'.format(np.round(end_time, 2)))\n",
        "    print()\n",
        "\n",
        "\n",
        "def pred_all_video():\n",
        "    # only process valid video files\n",
        "    valid_exts = ('.mp4', '.avi', '.mov', '.mkv')\n",
        "    path_all_videos = [\n",
        "        f for f in os.listdir(args.path_video)\n",
        "        if f.lower().endswith(valid_exts)\n",
        "    ]\n",
        "\n",
        "    if not path_all_videos:\n",
        "        print(\"âš ï¸ No valid video files found in\", args.path_video)\n",
        "        return\n",
        "\n",
        "    for id, cr_path in enumerate(path_all_videos):\n",
        "        print('{}/{}'.format(id + 1, len(path_all_videos)))\n",
        "        pred_one_video(os.path.join(args.path_video, cr_path))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pred_all_video()\n",
        "\n",
        "\"\"\"\n",
        "with open (\"/content/EMO-AffectNetModel/run.py\",\"w\") as fin:\n",
        "  fin.write(code)\n",
        "print(f\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSYSKruvYzRU",
        "outputId": "6cb59499-aaa7-4e7d-eec7-aeec2ab658b8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZd_uu9kPiwG",
        "outputId": "7b32d9fb-9de3-4882-9e3f-d2cd3814bf0f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EMO-AffectNetModel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --path_video \"/content/Manya/\" --path_save \"/content/Manya/Out\" --path_FE_model \"/content/EMO-AffectNetModel/weights_0_66_49_wo_gl.h5\" --path_LSTM_model \"/content/EMO-AffectNetModel/RAVDESS_with_config.h5\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vRk1c4LKCEo",
        "outputId": "d4df6998-fba0-45af-8aeb-116b63ce4936"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-12 14:49:25.461840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757688565.482025   19236 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757688565.488239   19236 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757688565.503664   19236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757688565.503690   19236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757688565.503693   19236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757688565.503696   19236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-12 14:49:25.508362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "1/1\n",
            "\n",
            "/content/Manya/IMG_7573.mp4\n",
            "Name video:  IMG_7573.mp4\n",
            "Number total of frames:  156\n",
            "FPS:  30.0\n",
            "Video duration: 5.2 s\n",
            "Frame width: 1080\n",
            "Frame height: 1920\n",
            "2025-09-12 14:49:58.758149: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1757688598.758340   19236 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13778 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "I0000 00:00:1757688600.967300   19236 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "âœ… Report saved in: /content/Manya/Out/IMG_7573.csv\n",
            "ðŸŽ­ Predicted emotion: Disgust\n",
            "â±ï¸ Lead time: 30.78 s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvtop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLz8uujjL4XX",
        "outputId": "ff94b5f3-97a1-4fba-a4d3-8d65bf5f448d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?1l\u001b>"
          ]
        }
      ]
    }
  ]
}